{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_lHlqACuufS"
   },
   "source": [
    "# Lab 2\n",
    "### Tabular RL on FrozenLake: Planning vs Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2_Fn9bnu2hK"
   },
   "source": [
    "#### Learning Goals\n",
    "\n",
    "By the end of this lab, you should be able to:\n",
    "\n",
    "* Describe an RL problem as an MDP: ⟨S, A, P, R, γ⟩\n",
    "\n",
    "* Compute an optimal value function using Value Iteration (planning; model-based)\n",
    "\n",
    "* Learn an optimal action-value function using Q-learning (learning; model-free)\n",
    "\n",
    "* Explain how both use the same core idea: Bellman optimality backups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run on Google Colab:\n",
    "\n",
    "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/duke-trust-lab/intro_modern_rl/blob/main/lab2/lab2.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "znG2eFUSu0B2"
   },
   "outputs": [],
   "source": [
    "%pip install -q gymnasium\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOVMVV7dvK_i"
   },
   "source": [
    "### Setting up a reproducible environment\n",
    "\n",
    "FrozenLake is a finite MDP, which makes it ideal for studying tabular methods.\n",
    "\n",
    "We set:\n",
    "\n",
    "`is_slippery=False` so transitions are deterministic\n",
    "* this isolates Bellman updates from stochastic noise\n",
    "* we will turn stochasticity back on later\n",
    "\n",
    "\n",
    "\n",
    "This environment gives us:\n",
    "\n",
    "A finite state space (S)\n",
    "\n",
    "A finite action space (A)\n",
    "\n",
    "A well-defined transition model P(s′ | s, a)\n",
    "\n",
    "A reward function R(s, a, s′)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1766116320310,
     "user": {
      "displayName": "Brinnae Bent",
      "userId": "15507223988771274310"
     },
     "user_tz": 300
    },
    "id": "Bm_skRyPuq7f",
    "outputId": "ea957472-c7e4-49b7-9c9b-fdabf69e479c"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Deterministic dynamics. We'll flip is_slippery=True later!\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"rgb_array\")\n",
    "\n",
    "_ = env.reset(seed=SEED)\n",
    "env.action_space.seed(SEED)\n",
    "env.observation_space.seed(SEED)\n",
    "\n",
    "nS = env.observation_space.n\n",
    "nA = env.action_space.n\n",
    "\n",
    "print(\"nS (states) =\", nS)\n",
    "print(\"nA (actions) =\", nA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_iHnsUFvPgE"
   },
   "source": [
    "### The MDP\n",
    "\n",
    "FrozenLake is a finite MDP:\n",
    "\n",
    "**States:** S = {0, 1, …, 15} (each tile in the 4×4 grid)\n",
    "\n",
    "**Actions:** A = {0, 1, 2, 3} (LEFT, DOWN, RIGHT, UP)\n",
    "\n",
    "**Transitions:** P(s′ | s, a)\n",
    "\n",
    "**Reward:** typically 1 when you reach the goal, else 0\n",
    "\n",
    "**Discount:** γ ∈ [0, 1)\n",
    "\n",
    "We are going to extract the environment’s transition model **P** and inspect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1766116433957,
     "user": {
      "displayName": "Brinnae Bent",
      "userId": "15507223988771274310"
     },
     "user_tz": 300
    },
    "id": "-tIHzYoJvHEh",
    "outputId": "93d2bea9-a2bf-47da-d5d0-d9f4bc6f39cb"
   },
   "outputs": [],
   "source": [
    "# In FrozenLake, env.unwrapped.P is a dict-like structure: P[s][a] = list of (prob, next_state, reward, terminated)\n",
    "P = env.unwrapped.P\n",
    "\n",
    "# sanity checks\n",
    "print(\"Keys in P (states):\", list(P.keys())[:5], \"...\")\n",
    "print(\"Example transitions P[0][0]:\", P[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZnqkOiS4vy2U"
   },
   "source": [
    "### Checkpoint\n",
    "\n",
    "\n",
    "> 1. What does each tuple (prob, next_state, reward, terminated) mean?\n",
    "\n",
    "> 2. Why is Value Iteration possible here, but not always in real-world RL?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1766116691365,
     "user": {
      "displayName": "Brinnae Bent",
      "userId": "15507223988771274310"
     },
     "user_tz": 300
    },
    "id": "cmjRkslPvinS",
    "outputId": "55dfb440-6ae3-4a20-b99e-7319feb49927"
   },
   "outputs": [],
   "source": [
    "def pretty_print_transitions(P, s, action_names=None):\n",
    "    \"\"\"\n",
    "    Print the transition dynamics for each action at state s.\n",
    "\n",
    "    P[s][a] is a list of tuples:\n",
    "      (probability, next_state, reward, terminated)\n",
    "    \"\"\"\n",
    "    if action_names is None:\n",
    "        action_names = {0: \"LEFT\", 1: \"DOWN\", 2: \"RIGHT\", 3: \"UP\"}\n",
    "\n",
    "    print(f\"State {s}\")\n",
    "    for a in P[s]:\n",
    "        action_label = action_names.get(a, f\"ACTION {a}\")\n",
    "        print(f\"  Action {action_label}:\")\n",
    "        for (p, s_next, r, done) in P[s][a]:\n",
    "            print(f\"    → (p={p:.2f}, s'={s_next}, r={r:.1f}, done={done})\")\n",
    "\n",
    "pretty_print_transitions(P, s=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEn_MhQLwqay"
   },
   "source": [
    "### Value Functions and Bellman Optimality\n",
    "\n",
    "We’ll use two value functions:\n",
    "\n",
    "State-value under a policy π:\n",
    "Vπ(s) = E[ Σₜ γᵗ rₜ | s₀ = s, π ]\n",
    "\n",
    "Optimal value:\n",
    "V*(s) = maxπ Vπ(s)\n",
    "\n",
    "Bellman optimality equation for V*:\n",
    "\n",
    "V*(s) = maxₐ Σₛ′ P(s′|s,a) [ R(s,a,s′) + γ V*(s′) ]\n",
    "\n",
    "*Value Iteration repeatedly applies this “backup” until values stop changing*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDVQ5EVD0y6z"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "This function implements one Bellman backup for a single state s\n",
    "\n",
    "Given:\n",
    "\n",
    "Current value estimates V\n",
    "\n",
    "Transition model P\n",
    "\n",
    "Discount factor γ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Eqcuey2wd3N"
   },
   "outputs": [],
   "source": [
    "def bellman_optimality_backup(P, V, s, gamma):\n",
    "    \"\"\"\n",
    "    Compute:\n",
    "      max_a Σ_s' P(s'|s,a) [ r + γ V(s') ]\n",
    "    TODO: implement this function.\n",
    "\n",
    "    Inputs:\n",
    "      P: transition model\n",
    "      V: np.array shape [nS]\n",
    "      s: int state\n",
    "      gamma: float\n",
    "\n",
    "    Returns:\n",
    "      v_new: float\n",
    "    \"\"\"\n",
    "    # TODO:\n",
    "    #   For each action a:\n",
    "    #     compute expected return = Σ over transitions (p * (r + gamma*V[s']))\n",
    "    #   return the max over a\n",
    "    v_new = 0.0\n",
    "    return v_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v2FVIAyK0-gz"
   },
   "source": [
    "This function repeatedly applies Bellman backups to all states:\n",
    "\n",
    "Initialize V(s) arbitrarily (zeros)\n",
    "\n",
    "For each state:\n",
    "\n",
    "Update V(s) using the Bellman optimality equation\n",
    "\n",
    "Stop when values stop changing (convergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1766117177544,
     "user": {
      "displayName": "Brinnae Bent",
      "userId": "15507223988771274310"
     },
     "user_tz": 300
    },
    "id": "A-19eya2yT9V",
    "outputId": "ab8508af-c06c-43ec-ae6c-374cebba9661"
   },
   "outputs": [],
   "source": [
    "def value_iteration(P, nS, nA, gamma=0.95, theta=1e-8, max_iters=10_000):\n",
    "    \"\"\"\n",
    "    Classic Value Iteration for V*.\n",
    "\n",
    "    TODO:\n",
    "      - implement the update for each state using bellman_optimality_backup\n",
    "      - stop when max change < theta\n",
    "\n",
    "    Returns:\n",
    "      V: optimal state-value np.array shape [nS]\n",
    "    \"\"\"\n",
    "    V = np.zeros(nS, dtype=np.float64)\n",
    "\n",
    "    for it in range(max_iters):\n",
    "        delta = 0.0\n",
    "\n",
    "        # TODO: loop over states s\n",
    "        #   v_old = V[s]\n",
    "        #   V[s] = bellman_optimality_backup(...)\n",
    "        #   delta = max(delta, abs(v_old - V[s]))\n",
    "\n",
    "        if delta < theta:\n",
    "            print(f\"Value Iteration converged in {it+1} iterations (delta={delta:.2e}).\")\n",
    "            break\n",
    "\n",
    "    return V\n",
    "\n",
    "gamma = 0.95\n",
    "V_star = value_iteration(P, nS, nA, gamma=gamma)\n",
    "V_star[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1uUHK_u1Oq1"
   },
   "source": [
    "Once we have V*(s), we compute:\n",
    "\n",
    "π*(s) = argmaxₐ Σₛ′ P(s′|s,a) [ r + γ V*(s′) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1766117192574,
     "user": {
      "displayName": "Brinnae Bent",
      "userId": "15507223988771274310"
     },
     "user_tz": 300
    },
    "id": "0wqEWX32yYLp",
    "outputId": "65c7b720-1d56-49d0-a398-c282b044e310"
   },
   "outputs": [],
   "source": [
    "def greedy_policy_from_V(P, V, nS, nA, gamma=0.95):\n",
    "    \"\"\"\n",
    "    π(s) = argmax_a Σ_s' P(s'|s,a) [ r + γ V(s') ]\n",
    "    TODO: implement.\n",
    "\n",
    "    Returns:\n",
    "      pi: np.array shape [nS] containing action index per state\n",
    "    \"\"\"\n",
    "    pi = np.zeros(nS, dtype=np.int64)\n",
    "\n",
    "    # TODO: fill pi[s] for each state s\n",
    "\n",
    "    return pi\n",
    "\n",
    "pi_vi = greedy_policy_from_V(P, V_star, nS, nA, gamma=gamma)\n",
    "pi_vi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uzDIqKUW1UB-"
   },
   "source": [
    "This function:\n",
    "\n",
    "Executes a fixed policy π in the environment\n",
    "\n",
    "Runs multiple episodes\n",
    "\n",
    "Returns the average reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 165,
     "status": "ok",
     "timestamp": 1766117206804,
     "user": {
      "displayName": "Brinnae Bent",
      "userId": "15507223988771274310"
     },
     "user_tz": 300
    },
    "id": "LjSEXukqybkQ",
    "outputId": "64c3ed76-709b-40fb-b92e-9ec3f5f5fdce"
   },
   "outputs": [],
   "source": [
    "def evaluate_policy(env, pi, episodes=200, max_steps=200):\n",
    "    returns = []\n",
    "    for _ in range(episodes):\n",
    "        s, _ = env.reset(seed=None)\n",
    "        total = 0.0\n",
    "        for _ in range(max_steps):\n",
    "            a = int(pi[s])\n",
    "            s, r, terminated, truncated, _ = env.step(a)\n",
    "            total += r\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        returns.append(total)\n",
    "    return float(np.mean(returns)), float(np.std(returns))\n",
    "\n",
    "mean_vi, std_vi = evaluate_policy(env, pi_vi, episodes=200)\n",
    "print(\"Value Iteration policy performance:\")\n",
    "print(\"  mean return =\", mean_vi, \"std =\", std_vi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MK6VYe7AyjWy"
   },
   "source": [
    "### Checkpoint\n",
    "\n",
    "> 1. Value Iteration never “plays the game” to learn. So how does it improve?\n",
    "\n",
    "\n",
    "> 2. What assumption does Value Iteration require that Q-learning does not?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGzk_dQsyxZi"
   },
   "source": [
    "### Q-learning (Learning / Model-Free Control)\n",
    "\n",
    "Q-learning learns an action-value table Q(s,a) using sampled experience\n",
    "\n",
    "The update is:\n",
    "\n",
    "Q(s,a) ← Q(s,a) + α [ r + γ maxₐ′ Q(s′,a′) − Q(s,a) ]\n",
    "\n",
    "This is a sampled Bellman optimality backup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5c-0nFJQ1iXv"
   },
   "source": [
    "---\n",
    "\n",
    "Q-learning maintains a table Q(s,a) and updates it via:\n",
    "\n",
    "Q(s,a) ← Q(s,a) + α [ r + γ maxₐ′ Q(s′,a′) − Q(s,a) ]\n",
    "\n",
    "\n",
    "This is a sampled Bellman optimality backup:\n",
    "\n",
    "No transition model needed\n",
    "\n",
    "Uses experience tuples (s, a, r, s′)\n",
    "\n",
    "---\n",
    "\n",
    "`epsilon_greedy_action`\n",
    "\n",
    "This function implements ε-greedy exploration:\n",
    "\n",
    "With probability ε: explore (random action)\n",
    "\n",
    "With probability 1−ε: exploit (best known action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KfCcFEyiyfSl"
   },
   "outputs": [],
   "source": [
    "Q = np.zeros((nS, nA), dtype=np.float64)\n",
    "\n",
    "def greedy_action(Q, s):\n",
    "    \"\"\"Return argmax_a Q(s,a).\"\"\"\n",
    "    return int(np.argmax(Q[s]))\n",
    "\n",
    "def epsilon_greedy_action(env, Q, s, epsilon):\n",
    "    \"\"\"\n",
    "    With probability epsilon: random action\n",
    "    Otherwise: greedy action\n",
    "\n",
    "    TODO: implement.\n",
    "    \"\"\"\n",
    "    # TODO:\n",
    "    #   if random.uniform(0,1) < epsilon: return env.action_space.sample()\n",
    "    #   else return greedy_action(Q,s)\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ozVrUbb3y-RN"
   },
   "outputs": [],
   "source": [
    "def epsilon_schedule(episode, eps_start=1.0, eps_min=0.05, decay_rate=0.0005):\n",
    "    \"\"\"\n",
    "    Exponential decay:\n",
    "      eps = max(eps_min, eps_start * exp(-decay_rate * episode))\n",
    "    TODO: implement.\n",
    "    \"\"\"\n",
    "    eps = eps_start\n",
    "    return eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnF3jHHS13DC"
   },
   "source": [
    "This is the full agent–environment loop:\n",
    "\n",
    "Reset environment\n",
    "\n",
    "Choose action via ε-greedy\n",
    "\n",
    "Observe (s′, r)\n",
    "\n",
    "Apply Q-learning update\n",
    "\n",
    "Repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238,
     "referenced_widgets": [
      "e9c47af18afc4f8d83ffe846669629eb",
      "9ccb1178ad0c4e86aae07d46e36e6de5",
      "ab0bdc85310c4a3f91af7c674b26f20e",
      "2c46b7269db741deb9e8b981c2998883",
      "fd30138aa2274c0fb37680799d74647c",
      "3caf414bf9cb476bb588b2527dbf7c3b",
      "e0b351d5cce4473883c9edba58aeb0f2",
      "ffd4658fe0164c69ad68891b94996a1e",
      "085ba51a4543496eb8b9a67327f74d7b",
      "16c4a4c6e3ef4706a12e06c56d960a4d",
      "3f15cc2b5a8a4af89c1b4c22a7b024b3"
     ]
    },
    "executionInfo": {
     "elapsed": 15270,
     "status": "ok",
     "timestamp": 1766117374731,
     "user": {
      "displayName": "Brinnae Bent",
      "userId": "15507223988771274310"
     },
     "user_tz": 300
    },
    "id": "byrc93M-zA5e",
    "outputId": "38fdb5f7-532d-4d2a-8084-917e7a0d189b"
   },
   "outputs": [],
   "source": [
    "def q_learning_train(env, Q, episodes=10_000, max_steps=100,\n",
    "                     alpha=0.7, gamma=0.95,\n",
    "                     eps_start=1.0, eps_min=0.05, decay_rate=0.0005):\n",
    "    \"\"\"\n",
    "    Train Q using Q-learning with epsilon-greedy exploration.\n",
    "\n",
    "    TODO:\n",
    "      - pick action using epsilon_greedy_action\n",
    "      - apply Q-learning update\n",
    "    \"\"\"\n",
    "    for ep in tqdm(range(episodes)):\n",
    "        epsilon = epsilon_schedule(ep, eps_start, eps_min, decay_rate)\n",
    "        s, _ = env.reset(seed=None)\n",
    "\n",
    "        for t in range(max_steps):\n",
    "            a = epsilon_greedy_action(env, Q, s, epsilon)\n",
    "            s2, r, terminated, truncated, _ = env.step(a)\n",
    "\n",
    "            # TODO: Q-learning update\n",
    "            # target = r + gamma * max_a' Q[s2, a']   (if not terminal; optional nuance)\n",
    "            # Q[s,a] <- Q[s,a] + alpha*(target - Q[s,a])\n",
    "            # Hint: np.max(Q[s2]) gives max over actions\n",
    "            # Q[s, a] = ...\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "            s = s2\n",
    "\n",
    "    return Q\n",
    "\n",
    "Q = np.zeros((nS, nA), dtype=np.float64)\n",
    "Q = q_learning_train(env, Q)\n",
    "pd.DataFrame(Q).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 209,
     "status": "ok",
     "timestamp": 1766117385179,
     "user": {
      "displayName": "Brinnae Bent",
      "userId": "15507223988771274310"
     },
     "user_tz": 300
    },
    "id": "bjl9bGK4zI0n",
    "outputId": "372e59d6-75fa-417e-a72f-39105c23c996"
   },
   "outputs": [],
   "source": [
    "pi_q = np.array([greedy_action(Q, s) for s in range(nS)], dtype=np.int64)\n",
    "mean_q, std_q = evaluate_policy(env, pi_q, episodes=200)\n",
    "print(\"Q-learning policy performance:\")\n",
    "print(\"  mean return =\", mean_q, \"std =\", std_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ji3pUERxzN67"
   },
   "source": [
    "### Checkpoint\n",
    "\n",
    "\n",
    "> 1. Why do we need ε-greedy at all?\n",
    "\n",
    "\n",
    "> 2. What do you think happens if ε is fixed at 0.0 from the start?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1766117487545,
     "user": {
      "displayName": "Brinnae Bent",
      "userId": "15507223988771274310"
     },
     "user_tz": 300
    },
    "id": "ylo_aDA8zK0w",
    "outputId": "867fc9c4-0e43-4b79-9e3e-fd83251059f6"
   },
   "outputs": [],
   "source": [
    "print(\"Comparison (deterministic FrozenLake, is_slippery=False)\")\n",
    "print(f\"  Value Iteration: mean={mean_vi:.3f}, std={std_vi:.3f}\")\n",
    "print(f\"  Q-learning:      mean={mean_q:.3f}, std={std_q:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84siEtK02IEU"
   },
   "source": [
    "This cell reshapes the optimal value function V*(s) from a 1-D table into the original 4×4 grid layout of FrozenLake, and displays it as a heatmap.\n",
    "\n",
    "Each cell in the grid corresponds to:\n",
    "\n",
    "A state in the environment\n",
    "\n",
    "The estimated expected discounted return starting from that state, assuming optimal behavior\n",
    "\n",
    "Darker (or brighter) colors indicate states with higher expected return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "executionInfo": {
     "elapsed": 415,
     "status": "ok",
     "timestamp": 1766117497856,
     "user": {
      "displayName": "Brinnae Bent",
      "userId": "15507223988771274310"
     },
     "user_tz": 300
    },
    "id": "KoYVxw_Xzj1G",
    "outputId": "163b03db-b590-40bd-b353-3dd6784b8e53"
   },
   "outputs": [],
   "source": [
    "V_grid = V_star.reshape(4,4)\n",
    "plt.figure()\n",
    "plt.title(\"V* from Value Iteration (reshaped to 4x4)\")\n",
    "plt.imshow(V_grid)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11_6dpGizqgJ"
   },
   "source": [
    "### Turn on Stochasticity\n",
    "\n",
    "Now `flip is_slippery=True`\n",
    "\n",
    "This introduces stochastic transitions. Your MDP model still exists, but learning becomes noisier and evaluation has variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1l5rquBRznpA"
   },
   "outputs": [],
   "source": [
    "env_slip = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True, render_mode=\"rgb_array\")\n",
    "_ = env_slip.reset(seed=SEED)\n",
    "env_slip.action_space.seed(SEED)\n",
    "env_slip.observation_space.seed(SEED)\n",
    "\n",
    "P_slip = env_slip.unwrapped.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118,
     "referenced_widgets": [
      "d9df97d2e8c54884a66c6d1cbb70d990",
      "774a51a9e7ef4a57a0cbdb6abed942c5",
      "f7c458b536724b89851350ac56b81e9c",
      "00176350275b48b1b7e5893bbabdc607",
      "7a5de202acb94edbad7bd30676e99bde",
      "d9d8083d92c94d0caeca748fbe8fbe60",
      "5c3ace9494174d1faeee1acc9c2f9050",
      "6f79de2dc1fb4dc79aa2268fafe48a75",
      "1170a3aa1d1f47f9952fe84793b918f1",
      "03763bd7410647c1bc199dbf769a1b49",
      "bfec4f5d75e443d393e59141c967a8f9"
     ]
    },
    "executionInfo": {
     "elapsed": 8291,
     "status": "ok",
     "timestamp": 1766117574180,
     "user": {
      "displayName": "Brinnae Bent",
      "userId": "15507223988771274310"
     },
     "user_tz": 300
    },
    "id": "7KHR1SHhzzdy",
    "outputId": "c1345504-935a-4863-b832-7760c82c1b40"
   },
   "outputs": [],
   "source": [
    "# Value Iteration policy on slippery dynamics\n",
    "V_star_slip = value_iteration(P_slip, nS, nA, gamma=gamma)\n",
    "pi_vi_slip = greedy_policy_from_V(P_slip, V_star_slip, nS, nA, gamma=gamma)\n",
    "mean_vi_s, std_vi_s = evaluate_policy(env_slip, pi_vi_slip, episodes=500)\n",
    "\n",
    "# Q-learning on slippery dynamics\n",
    "Q_slip = np.zeros((nS, nA), dtype=np.float64)\n",
    "Q_slip = q_learning_train(env_slip, Q_slip, episodes=20_000)  # more episodes helps\n",
    "pi_q_slip = np.array([greedy_action(Q_slip, s) for s in range(nS)], dtype=np.int64)\n",
    "mean_q_s, std_q_s = evaluate_policy(env_slip, pi_q_slip, episodes=500)\n",
    "\n",
    "print(\"Comparison (stochastic FrozenLake, is_slippery=True)\")\n",
    "print(f\"  Value Iteration: mean={mean_vi_s:.3f}, std={std_vi_s:.3f}\")\n",
    "print(f\"  Q-learning:      mean={mean_q_s:.3f}, std={std_q_s:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPQv8TbK8PIxPluPBoA7zMZ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00176350275b48b1b7e5893bbabdc607": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_03763bd7410647c1bc199dbf769a1b49",
      "placeholder": "​",
      "style": "IPY_MODEL_bfec4f5d75e443d393e59141c967a8f9",
      "value": " 20000/20000 [00:08&lt;00:00, 3170.89it/s]"
     }
    },
    "03763bd7410647c1bc199dbf769a1b49": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "085ba51a4543496eb8b9a67327f74d7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1170a3aa1d1f47f9952fe84793b918f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "16c4a4c6e3ef4706a12e06c56d960a4d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2c46b7269db741deb9e8b981c2998883": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_16c4a4c6e3ef4706a12e06c56d960a4d",
      "placeholder": "​",
      "style": "IPY_MODEL_3f15cc2b5a8a4af89c1b4c22a7b024b3",
      "value": " 10000/10000 [00:15&lt;00:00, 659.01it/s]"
     }
    },
    "3caf414bf9cb476bb588b2527dbf7c3b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f15cc2b5a8a4af89c1b4c22a7b024b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5c3ace9494174d1faeee1acc9c2f9050": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6f79de2dc1fb4dc79aa2268fafe48a75": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "774a51a9e7ef4a57a0cbdb6abed942c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d9d8083d92c94d0caeca748fbe8fbe60",
      "placeholder": "​",
      "style": "IPY_MODEL_5c3ace9494174d1faeee1acc9c2f9050",
      "value": "100%"
     }
    },
    "7a5de202acb94edbad7bd30676e99bde": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9ccb1178ad0c4e86aae07d46e36e6de5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3caf414bf9cb476bb588b2527dbf7c3b",
      "placeholder": "​",
      "style": "IPY_MODEL_e0b351d5cce4473883c9edba58aeb0f2",
      "value": "100%"
     }
    },
    "ab0bdc85310c4a3f91af7c674b26f20e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ffd4658fe0164c69ad68891b94996a1e",
      "max": 10000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_085ba51a4543496eb8b9a67327f74d7b",
      "value": 10000
     }
    },
    "bfec4f5d75e443d393e59141c967a8f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d9d8083d92c94d0caeca748fbe8fbe60": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d9df97d2e8c54884a66c6d1cbb70d990": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_774a51a9e7ef4a57a0cbdb6abed942c5",
       "IPY_MODEL_f7c458b536724b89851350ac56b81e9c",
       "IPY_MODEL_00176350275b48b1b7e5893bbabdc607"
      ],
      "layout": "IPY_MODEL_7a5de202acb94edbad7bd30676e99bde"
     }
    },
    "e0b351d5cce4473883c9edba58aeb0f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e9c47af18afc4f8d83ffe846669629eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9ccb1178ad0c4e86aae07d46e36e6de5",
       "IPY_MODEL_ab0bdc85310c4a3f91af7c674b26f20e",
       "IPY_MODEL_2c46b7269db741deb9e8b981c2998883"
      ],
      "layout": "IPY_MODEL_fd30138aa2274c0fb37680799d74647c"
     }
    },
    "f7c458b536724b89851350ac56b81e9c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6f79de2dc1fb4dc79aa2268fafe48a75",
      "max": 20000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1170a3aa1d1f47f9952fe84793b918f1",
      "value": 20000
     }
    },
    "fd30138aa2274c0fb37680799d74647c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ffd4658fe0164c69ad68891b94996a1e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
