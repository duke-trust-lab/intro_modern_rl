{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWhwRbp1LzM6"
   },
   "source": [
    "# Lab 4\n",
    "## CartPole with Value-Based Agents (DQN Variants)\n",
    "\n",
    "Objectives:\n",
    "1. Train a **baseline DQN** on CartPole using **RL Zoo 3** (a training harness around Stable-Baselines3).\n",
    "2. Diagnose **stability** by inspecting learning curves and evaluation performance.\n",
    "3. Implement and train **Dueling DQN** (an architecture change) using Stable-Baselines3 directly.\n",
    "4. On your own, implemnet **Double DQN**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run on Google Colab:\n",
    "\n",
    "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/duke-trust-lab/intro_modern_rl/blob/main/lab4/lab4.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12425,
     "status": "ok",
     "timestamp": 1766890433520,
     "user": {
      "displayName": "Brinnae Bent",
      "userId": "15507223988771274310"
     },
     "user_tz": 300
    },
    "id": "cwsRWZIqL0CB",
    "outputId": "a02c2901-637f-455c-caf9-1a93534f14e8"
   },
   "outputs": [],
   "source": [
    "%pip install -q stable-baselines3==2.* sb3-contrib==2.* gymnasium matplotlib pandas pyyaml imageio ffmpeg-python\n",
    "%pip install -q rl_zoo3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4844,
     "status": "ok",
     "timestamp": 1766891075298,
     "user": {
      "displayName": "Brinnae Bent",
      "userId": "15507223988771274310"
     },
     "user_tz": 300
    },
    "id": "wZxDQdaTL0FS"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import Video, display\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ve5xS5SMD2u"
   },
   "source": [
    "### What DQN is doing\n",
    "\n",
    "DQN learns an approximation of the action-value function:\n",
    "\n",
    "Qθ(s,a)\n",
    "\n",
    "A standard DQN target is:\n",
    "\n",
    "y = r + γ max_a' Qθ⁻(s', a')\n",
    "\n",
    "where:\n",
    "- θ: online network parameters (updated every gradient step)\n",
    "- θ⁻: target network parameters (updated more slowly)\n",
    "\n",
    "DQN typically uses:\n",
    "- **experience replay**: sample random mini-batches from a replay buffer (reduces correlation)\n",
    "- **target network**: stabilizes the moving target in TD learning\n",
    "- **ε-greedy exploration**: balance exploration vs exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ftqh6bjrMI9X"
   },
   "source": [
    "---\n",
    "\n",
    "## Why use RL Zoo 3 for the baseline run?\n",
    "\n",
    "RL Zoo provides:\n",
    "- a clean, repeatable CLI training workflow\n",
    "- standardized logging + saved best models\n",
    "- video recording utilities\n",
    "\n",
    "This lets us focus on:\n",
    "- what the hyperparameters mean\n",
    "- what the training curves tell us\n",
    "- how evaluation differs from training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQo3wE6rMR9j"
   },
   "source": [
    "### Create RL Zoo YAML config for baseline DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1766891075312,
     "user": {
      "displayName": "Brinnae Bent",
      "userId": "15507223988771274310"
     },
     "user_tz": 300
    },
    "id": "xveUFRxZL0IS",
    "outputId": "a562a95a-2259-4465-a94b-61f67e7e1ca1"
   },
   "outputs": [],
   "source": [
    "ENV_ID = \"CartPole-v1\"\n",
    "LOG_DIR = \"logs\"\n",
    "\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "dqn_config = {\n",
    "    ENV_ID: {\n",
    "        \"policy\": \"MlpPolicy\",\n",
    "        \"n_timesteps\": float(6e4),\n",
    "        \"buffer_size\": 20000,\n",
    "        \"learning_starts\": 1000,\n",
    "        \"target_update_interval\": 250,\n",
    "        \"exploration_fraction\": 0.2,\n",
    "        \"exploration_final_eps\": 0.06,\n",
    "        \"learning_rate\": float(1e-3),\n",
    "        \"batch_size\": 64,\n",
    "        \"train_freq\": 32,\n",
    "        \"gradient_steps\": 16,\n",
    "        \"gamma\": 0.99,\n",
    "        \"optimize_memory_usage\": False,\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"dqn_base.yaml\", \"w\") as f:\n",
    "    yaml.safe_dump(dqn_config, f, sort_keys=False)\n",
    "\n",
    "print(\"Wrote dqn_base.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 119290,
     "status": "ok",
     "timestamp": 1766891197120,
     "user": {
      "displayName": "Brinnae Bent",
      "userId": "15507223988771274310"
     },
     "user_tz": 300
    },
    "id": "gFgeSMSdL0Ky",
    "outputId": "b63db55a-d0d9-41f1-f906-2de6ed17ceea"
   },
   "outputs": [],
   "source": [
    "# Train a single baseline run (seed 0). You can later expand to multiple seeds.\n",
    "seed = 0\n",
    "cmd = f\"python -m rl_zoo3.train --algo dqn --env {ENV_ID} -f {LOG_DIR}/ -c dqn_base.yaml --log-interval 1000 --seed {seed}\"\n",
    "print(\"Running:\", cmd)\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16348,
     "status": "ok",
     "timestamp": 1766891213474,
     "user": {
      "displayName": "Brinnae Bent",
      "userId": "15507223988771274310"
     },
     "user_tz": 300
    },
    "id": "6txALFFtL0NF",
    "outputId": "7ebc959b-b7d9-4f2e-89ab-b7ba425779cf"
   },
   "outputs": [],
   "source": [
    "cmd = f\"\"\"python -m rl_zoo3.record_video \\\n",
    "  --algo dqn \\\n",
    "  --env {ENV_ID} \\\n",
    "  --folder {LOG_DIR}/ \\\n",
    "  -n 600 \\\n",
    "  --deterministic \\\n",
    "  --load-best\n",
    "\"\"\"\n",
    "print(\"Running:\", cmd)\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1766460285459,
     "user": {
      "displayName": "Brinnae Bent",
      "userId": "15507223988771274310"
     },
     "user_tz": 300
    },
    "id": "3S3-WrzSL0R9",
    "outputId": "7d97509d-9d7a-4623-b0d3-7c90183b385c"
   },
   "outputs": [],
   "source": [
    "mp4s = glob.glob(os.path.join(LOG_DIR, \"**\", \"*.mp4\"), recursive=True)\n",
    "mp4s_sorted = sorted(mp4s, key=os.path.getmtime, reverse=True)\n",
    "print(\"Found videos:\", len(mp4s_sorted))\n",
    "\n",
    "if mp4s_sorted:\n",
    "    print(\"Showing:\", mp4s_sorted[0])\n",
    "    display(Video(mp4s_sorted[0], embed=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7QaY__cMhYO"
   },
   "source": [
    "## Diagnosing value estimation instability\n",
    "\n",
    "In deep value-based RL, instability can show up as:\n",
    "- learning curves that spike then collapse\n",
    "- large oscillations in evaluation returns\n",
    "- sensitivity to seed / hyperparameters\n",
    "- apparently “good” training signals that don’t transfer to evaluation behavior\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3c3E-VcEMwRu"
   },
   "source": [
    "### Dueling DQN\n",
    "\n",
    "Dueling DQN explicitly decomposes:\n",
    "\n",
    "Q(s,a) = V(s) + A(s,a) − mean_a A(s,a)\n",
    "\n",
    "Intuition:\n",
    "- V(s): how good is the state (regardless of action)?\n",
    "- A(s,a): how much better is action a compared to other actions in state s?\n",
    "\n",
    "This can help in environments where:\n",
    "- many actions have similar value\n",
    "- learning V(s) is easier than learning full Q(s,a) directly\n",
    "\n",
    "We will implement a custom DQN policy with dueling heads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5237,
     "status": "ok",
     "timestamp": 1766891250825,
     "user": {
      "displayName": "Brinnae Bent",
      "userId": "15507223988771274310"
     },
     "user_tz": 300
    },
    "id": "NCzqTi8TMfbU",
    "outputId": "be9382e0-d49a-4a5e-dce0-19b06c7772d1"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "SB3_LOG_BASE = \"sb3_logs\"\n",
    "os.makedirs(SB3_LOG_BASE, exist_ok=True)\n",
    "\n",
    "def make_env(env_id=\"CartPole-v1\"):\n",
    "    env = gym.make(env_id)\n",
    "    env = Monitor(env)\n",
    "    return env\n",
    "\n",
    "def train_sb3(\n",
    "    name: str,\n",
    "    policy,\n",
    "    env_id=\"CartPole-v1\",\n",
    "    total_timesteps=60_000,\n",
    "    seed=0,\n",
    "    policy_kwargs=None,\n",
    "):\n",
    "    env = make_env(env_id)\n",
    "    eval_env = make_env(env_id)\n",
    "\n",
    "    run_dir = os.path.join(SB3_LOG_BASE, name)\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "    eval_callback = EvalCallback(\n",
    "        eval_env,\n",
    "        best_model_save_path=run_dir,\n",
    "        log_path=run_dir,\n",
    "        eval_freq=10_000,\n",
    "        deterministic=True,\n",
    "        render=False,\n",
    "    )\n",
    "\n",
    "    model = DQN(\n",
    "        policy,\n",
    "        env,\n",
    "        learning_rate=1e-3,\n",
    "        buffer_size=20_000,\n",
    "        learning_starts=1_000,\n",
    "        batch_size=64,\n",
    "        tau=1.0,                  # hard target updates\n",
    "        target_update_interval=250,\n",
    "        train_freq=32,\n",
    "        gradient_steps=16,\n",
    "        gamma=0.99,\n",
    "        exploration_fraction=0.2,\n",
    "        exploration_final_eps=0.06,\n",
    "        policy_kwargs=policy_kwargs,\n",
    "        verbose=0,\n",
    "        seed=seed,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    )\n",
    "\n",
    "    model.learn(total_timesteps=total_timesteps, callback=eval_callback)\n",
    "    model.save(os.path.join(run_dir, \"final_model\"))\n",
    "    return run_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gkods3iNGAL"
   },
   "source": [
    "### Define Dueling DQN policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 56,
     "status": "ok",
     "timestamp": 1766891261411,
     "user": {
      "displayName": "Brinnae Bent",
      "userId": "15507223988771274310"
     },
     "user_tz": 300
    },
    "id": "sQms9pQFM_-N"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from stable_baselines3.dqn.policies import DQNPolicy, QNetwork\n",
    "\n",
    "class DuelingQNetwork(QNetwork):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        hidden_dim = 256\n",
    "\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(self.features_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Value head V(s)\n",
    "        self.V = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "\n",
    "        # Advantage head A(s,a)\n",
    "        self.A = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, self.action_space.n),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        features = self.extract_features(obs, self.features_extractor)\n",
    "        h = self.shared(features)\n",
    "        v = self.V(h)                          # [batch, 1]\n",
    "        a = self.A(h)                          # [batch, n_actions]\n",
    "        q = v + (a - a.mean(dim=1, keepdim=True))\n",
    "        return q\n",
    "\n",
    "class DuelingDQNPolicy(DQNPolicy):\n",
    "    def make_q_net(self):\n",
    "        net_args = self._update_features_extractor(self.net_args, features_extractor=None)\n",
    "        return DuelingQNetwork(**net_args).to(self.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4npRdmNoNLUp"
   },
   "source": [
    "### Train baseline SB3 DQN + Dueling DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 243460,
     "status": "ok",
     "timestamp": 1766891508155,
     "user": {
      "displayName": "Brinnae Bent",
      "userId": "15507223988771274310"
     },
     "user_tz": 300
    },
    "id": "tGhabRRWNAAt",
    "outputId": "028f80ff-1e4e-4db0-de18-9c3af45e0b4f"
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "sb3_baseline_dir = train_sb3(\"dqn_baseline_seed0\", policy=\"MlpPolicy\", seed=seed)\n",
    "sb3_dueling_dir  = train_sb3(\"dqn_dueling_seed0\",  policy=DuelingDQNPolicy, seed=seed)\n",
    "\n",
    "sb3_baseline_dir, sb3_dueling_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVGLH1WKNRGQ"
   },
   "source": [
    "### SB3 eval curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 222,
     "status": "ok",
     "timestamp": 1766891508387,
     "user": {
      "displayName": "Brinnae Bent",
      "userId": "15507223988771274310"
     },
     "user_tz": 300
    },
    "id": "SuYaQHVzMfdt",
    "outputId": "989304f7-d354-4f4e-f7fd-86fd58be5c5a"
   },
   "outputs": [],
   "source": [
    "def load_eval_npz(run_dir):\n",
    "    npz_path = os.path.join(run_dir, \"evaluations.npz\")\n",
    "    if not os.path.exists(npz_path):\n",
    "        return None\n",
    "    data = np.load(npz_path)\n",
    "    timesteps = data[\"timesteps\"]\n",
    "    results = data[\"results\"]          # shape [n_evals, n_episodes]\n",
    "    mean_reward = results.mean(axis=1)\n",
    "    return pd.DataFrame({\"timesteps\": timesteps, \"mean_reward\": mean_reward})\n",
    "\n",
    "b = load_eval_npz(sb3_baseline_dir)\n",
    "d = load_eval_npz(sb3_dueling_dir)\n",
    "\n",
    "plt.figure()\n",
    "if b is not None:\n",
    "    plt.plot(b[\"timesteps\"], b[\"mean_reward\"], label=\"SB3 DQN baseline\")\n",
    "if d is not None:\n",
    "    plt.plot(d[\"timesteps\"], d[\"mean_reward\"], label=\"SB3 Dueling DQN\")\n",
    "plt.xlabel(\"timesteps\")\n",
    "plt.ylabel(\"eval mean reward\")\n",
    "plt.title(\"Evaluation Performance: Baseline vs Dueling\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1766891519789,
     "user": {
      "displayName": "Brinnae Bent",
      "userId": "15507223988771274310"
     },
     "user_tz": 300
    },
    "id": "uteBPyJyRYWq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import imageio\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "def record_sb3_rollout_mp4(\n",
    "    model_zip_path: str,\n",
    "    out_mp4_path: str,\n",
    "    env_id: str = \"CartPole-v1\",\n",
    "    n_steps: int = 600,          # 600 frames at 30 fps = 20 seconds\n",
    "    fps: int = 30,\n",
    "    deterministic: bool = True,\n",
    "):\n",
    "    os.makedirs(os.path.dirname(out_mp4_path), exist_ok=True)\n",
    "\n",
    "    env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "    model = DQN.load(model_zip_path)\n",
    "\n",
    "    obs, info = env.reset()\n",
    "    frames = []\n",
    "\n",
    "    for _ in range(n_steps):\n",
    "        frame = env.render()\n",
    "        frames.append(frame)\n",
    "\n",
    "        action, _ = model.predict(obs, deterministic=deterministic)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        if terminated or truncated:\n",
    "            obs, info = env.reset()\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    imageio.mimsave(out_mp4_path, frames, fps=fps)\n",
    "    return out_mp4_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5609,
     "status": "ok",
     "timestamp": 1766891529128,
     "user": {
      "displayName": "Brinnae Bent",
      "userId": "15507223988771274310"
     },
     "user_tz": 300
    },
    "id": "3nBrx5jyRVBo",
    "outputId": "91fa2ec6-f1c8-4199-d7f4-6304496b060a"
   },
   "outputs": [],
   "source": [
    "baseline_mp4 = record_sb3_rollout_mp4(\n",
    "    model_zip_path=os.path.join(sb3_baseline_dir, \"best_model.zip\"),\n",
    "    out_mp4_path=os.path.join(sb3_baseline_dir, \"videos_manual\", \"baseline.mp4\"),\n",
    "    n_steps=600,\n",
    "    fps=30,\n",
    ")\n",
    "\n",
    "dueling_mp4 = record_sb3_rollout_mp4(\n",
    "    model_zip_path=os.path.join(sb3_dueling_dir, \"best_model.zip\"),\n",
    "    out_mp4_path=os.path.join(sb3_dueling_dir, \"videos_manual\", \"dueling.mp4\"),\n",
    "    n_steps=600,\n",
    "    fps=30,\n",
    ")\n",
    "\n",
    "baseline_mp4, dueling_mp4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 825
    },
    "executionInfo": {
     "elapsed": 48,
     "status": "ok",
     "timestamp": 1766891529175,
     "user": {
      "displayName": "Brinnae Bent",
      "userId": "15507223988771274310"
     },
     "user_tz": 300
    },
    "id": "-c-mfiqFRaGj",
    "outputId": "7114b289-f24d-4006-feba-d9504bb4b122"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Video, display\n",
    "\n",
    "display(Video(baseline_mp4, embed=True))\n",
    "display(Video(dueling_mp4, embed=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_mebocg8osz"
   },
   "source": [
    "### Checkpoint\n",
    "\n",
    "**What happened?**\n",
    "\n",
    "\n",
    "Dueling DQN learns faster at first because it can more easily estimate which states are good. But that same decomposition makes it more sensitive to noisy value estimates. Without Double DQN or careful tuning, those errors can compound and cause instability. Baseline DQN is slower but more robust here, which is why simple methods often outperform more advanced ones on small problems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktGEhGQTNeuK"
   },
   "source": [
    "### Double DQN\n",
    "\n",
    "A common failure mode in value-based RL is **overestimation bias**.\n",
    "\n",
    "Why?\n",
    "- Q-values are learned estimates and are noisy.\n",
    "- Taking `max_a Q(s,a)` tends to select actions with positive noise.\n",
    "\n",
    "Baseline DQN target:\n",
    "y = r + γ max_a' Qθ⁻(s', a')\n",
    "\n",
    "Double DQN target (key idea: decouple selection and evaluation):\n",
    "1) select action using online net:\n",
    "   a* = argmax_a' Qθ(s', a')\n",
    "\n",
    "2) evaluate that action using target net:\n",
    "   y = r + γ Qθ⁻(s', a*)\n",
    "\n",
    "This reduces maximization bias and often improves stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SY1f8FCZ3PtQ"
   },
   "source": [
    "### TODO\n",
    "- Implement Double DQN\n",
    "- Plot evaluation of Baseline v. Dueling v. Double DQN\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMAN8cytR4dFEBLe9dX/MVe",
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
